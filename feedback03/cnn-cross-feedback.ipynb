{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Gfu5V2bU_Hqj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Fw4gm_qrEPy_"
   },
   "outputs": [],
   "source": [
    "cuda0 = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-529eNn_Hqk"
   },
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-weight:bold\">\n",
    "Comment: equal to the sample solution. A plot to visualize the training progress would be nice (also for the following tasks) but this is also missing in the sample solution.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HbBK4q-n_Hql"
   },
   "outputs": [],
   "source": [
    "mb_size = 100 # mini-batch size of 100\n",
    "\n",
    "\n",
    "trans = transforms.Compose([transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "\n",
    "dataset = dset.MNIST(\"./\", download = True,\n",
    "                     train = True,\n",
    "                     transform = trans)\n",
    "\n",
    "test_dataset = dset.MNIST(\"./\", download=True,\n",
    "                          train=False,\n",
    "                          transform = trans)\n",
    "\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=mb_size,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=mb_size,\n",
    "                                          shuffle=True, num_workers=1,\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8Y3LSxAB_Hqm"
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    # Kaiming He initialization (a good initialization is important)\n",
    "    # https://arxiv.org/abs/1502.01852\n",
    "    std = np.sqrt(2. / shape[0])\n",
    "    w = torch.randn(size=shape, device=cuda0) * std\n",
    "    w.requires_grad = True\n",
    "\n",
    "    return w.to(cuda0)\n",
    "\n",
    "\n",
    "def rectify(X):\n",
    "    return torch.max(torch.zeros_like(X, device=cuda0), X).to(cuda0)\n",
    "\n",
    "\n",
    "# this is an example as a reduced version of the pytorch internal RMSprop optimizer\n",
    "class RMSprop(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.5, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(grad, grad, value=1-alpha)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(grad, avg, value=-group['lr'])\n",
    "\n",
    "\n",
    "def model(X, w_h, w_h2, w_o):\n",
    "    h = rectify(X @ w_h)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0GavQK8P_Hqm",
    "outputId": "8dce95e2-13a8-4b85-8b38-a6f24bd7357b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Average Train Loss: 0.40141400694847107\n",
      "Average Test Loss: 0.2639123797416687\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Average Train Loss: 0.15982545912265778\n",
      "Average Test Loss: 0.23186850547790527\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Average Train Loss: 0.10280334204435349\n",
      "Average Test Loss: 0.3892219364643097\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Average Train Loss: 0.08715856820344925\n",
      "Average Test Loss: 0.616568386554718\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Epoch: 41\n",
      "Average Train Loss: 0.05260054022073746\n",
      "Average Test Loss: 0.6245554089546204\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Epoch: 51\n",
      "Average Train Loss: 0.0411297045648098\n",
      "Average Test Loss: 0.7247546911239624\n",
      "Epoch: 52\n",
      "Epoch: 53\n",
      "Epoch: 54\n",
      "Epoch: 55\n",
      "Epoch: 56\n",
      "Epoch: 57\n",
      "Epoch: 58\n",
      "Epoch: 59\n",
      "Epoch: 60\n",
      "Epoch: 61\n",
      "Average Train Loss: 0.0365484356880188\n",
      "Average Test Loss: 0.6949684023857117\n",
      "Epoch: 62\n",
      "Epoch: 63\n",
      "Epoch: 64\n",
      "Epoch: 65\n",
      "Epoch: 66\n",
      "Epoch: 67\n",
      "Epoch: 68\n",
      "Epoch: 69\n",
      "Epoch: 70\n",
      "Epoch: 71\n",
      "Average Train Loss: 0.01929316483438015\n",
      "Average Test Loss: 0.74756920337677\n",
      "Epoch: 72\n",
      "Epoch: 73\n",
      "Epoch: 74\n",
      "Epoch: 75\n",
      "Epoch: 76\n",
      "Epoch: 77\n",
      "Epoch: 78\n",
      "Epoch: 79\n",
      "Epoch: 80\n",
      "Epoch: 81\n",
      "Average Train Loss: 0.011383159086108208\n",
      "Average Test Loss: 0.7894361615180969\n",
      "Epoch: 82\n",
      "Epoch: 83\n",
      "Epoch: 84\n",
      "Epoch: 85\n",
      "Epoch: 86\n",
      "Epoch: 87\n",
      "Epoch: 88\n",
      "Epoch: 89\n",
      "Epoch: 90\n",
      "Epoch: 91\n",
      "Average Train Loss: 0.01677250675857067\n",
      "Average Test Loss: 0.8410241007804871\n",
      "Epoch: 92\n",
      "Epoch: 93\n",
      "Epoch: 94\n",
      "Epoch: 95\n",
      "Epoch: 96\n",
      "Epoch: 97\n",
      "Epoch: 98\n",
      "Epoch: 99\n",
      "Epoch: 100\n",
      "Epoch: 101\n",
      "Average Train Loss: 0.017852241173386574\n",
      "Average Test Loss: 0.8754834532737732\n"
     ]
    }
   ],
   "source": [
    "w_h = init_weights((784, 625))\n",
    "w_h2 = init_weights((625, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])\n",
    "\n",
    "\n",
    "# put this into a training loop over 100 epochs\n",
    "for i in range(101):\n",
    "    print(\"Epoch: {}\".format(i+1))\n",
    "    avg_train_loss = 0.\n",
    "    for (j, (X, y)) in enumerate(dataloader):\n",
    "        noise_py_x = model(X.reshape(mb_size, 784).to(cuda0), w_h, w_h2, w_o).to(cuda0)\n",
    "        optimizer.zero_grad()\n",
    "        # the cross-entropy loss function already contains the softmax\n",
    "        cost = torch.nn.functional.cross_entropy(noise_py_x, y.to(cuda0), reduction=\"mean\").to(cuda0)\n",
    "        avg_train_loss += cost\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(\"Average Train Loss: {}\".format(avg_train_loss / (j + 1)))\n",
    "\n",
    "        # no need to calculate gradients for validation\n",
    "        with torch.no_grad():\n",
    "            avg_test_loss = 0.\n",
    "            for (k, (X, y)) in enumerate(test_loader):\n",
    "                noise_py_x = model(X.reshape(mb_size, 784).to(cuda0), w_h, w_h2, w_o).to(cuda0)\n",
    "                cost = torch.nn.functional.cross_entropy(noise_py_x, y.to(cuda0), reduction=\"mean\").to(cuda0)\n",
    "                avg_test_loss += cost\n",
    "\n",
    "            print(\"Average Test Loss: {}\".format(avg_test_loss / (k + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ay-7TCpw_Hqn"
   },
   "source": [
    "# 2 Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-weight:bold\">\n",
    "Comment: This is a nice way to calculate the dropout, since you don't have to use numpy as in the sample solution. The answers to the questions seem good and correct. The rest is essentially equal to the sample solution.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UE_m7h7J_Hqo"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "\n",
    "def dropout(X, p_drop= 0.5):\n",
    "    \n",
    "    if 0 < p_drop < 1:\n",
    "        binomial = torch.distributions.binomial.Binomial(probs=1-p_drop)\n",
    "        return X * binomial.sample(X.size()).to(cuda0) * (1.0/(1-p_drop))\n",
    "    else:\n",
    "        return X\n",
    "    \n",
    "def dropout_model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    \n",
    "    h = rectify(dropout(X, p_drop_input) @ w_h)\n",
    "    h2 = rectify(dropout(h, p_drop_hidden) @ w_h2)\n",
    "    pre_softmax = dropout(h2, p_drop_hidden)  @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7NnntRI_Hqo"
   },
   "source": [
    "Dropout increases robustness and reduces overfitting due to the network always training on a random subset of neurons. This reduces the potential overreliance of the network on only a couple single neurons. Without dropout we often see many co-adaptions in networks, meaining that some neurons cancel out the mistakes of others. This in general does to generalize well; dropout prevents this as the network can not rely on all neurons being present at all the time, significantly reducing the amount of co-adaption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTAw7ASa_Hqo"
   },
   "source": [
    "For the test-set we obviously do not want to use the dropout model, as we want to test the model on all the activations. At the same time we need to keep in mind that the network activations are now much larger that the networks was used to during training (since we are using all the neuros at once now. A simple fix is to downscale the weights after training by multiplying them with 1-p_drop.\n",
    "Here we realize this by including the scaling directly in the dropout training model (inverted dropout), allowing us to use the simple model during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b8mPgd2_Hqp",
    "outputId": "bedb7958-bb75-4826-92bd-3ca99ae5dbf0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Average Train Loss: 1.0107272863388062\n",
      "Average Test Loss: 0.32930248975753784\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Average Train Loss: 0.9989118576049805\n",
      "Average Test Loss: 0.30084410309791565\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Average Train Loss: 1.2981696128845215\n",
      "Average Test Loss: 0.3829096257686615\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Average Train Loss: 1.5067752599716187\n",
      "Average Test Loss: 0.4098247289657593\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Epoch: 41\n",
      "Average Train Loss: 1.7145969867706299\n",
      "Average Test Loss: 0.5594747066497803\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Epoch: 51\n",
      "Average Train Loss: 1.853873372077942\n",
      "Average Test Loss: 0.6022546291351318\n",
      "Epoch: 52\n",
      "Epoch: 53\n",
      "Epoch: 54\n",
      "Epoch: 55\n",
      "Epoch: 56\n",
      "Epoch: 57\n",
      "Epoch: 58\n",
      "Epoch: 59\n",
      "Epoch: 60\n",
      "Epoch: 61\n",
      "Average Train Loss: 2.015296459197998\n",
      "Average Test Loss: 0.6797450184822083\n",
      "Epoch: 62\n",
      "Epoch: 63\n",
      "Epoch: 64\n",
      "Epoch: 65\n",
      "Epoch: 66\n",
      "Epoch: 67\n",
      "Epoch: 68\n",
      "Epoch: 69\n",
      "Epoch: 70\n",
      "Epoch: 71\n",
      "Average Train Loss: 2.0353596210479736\n",
      "Average Test Loss: 0.652331531047821\n",
      "Epoch: 72\n",
      "Epoch: 73\n",
      "Epoch: 74\n",
      "Epoch: 75\n",
      "Epoch: 76\n",
      "Epoch: 77\n",
      "Epoch: 78\n",
      "Epoch: 79\n",
      "Epoch: 80\n",
      "Epoch: 81\n",
      "Average Train Loss: 2.0741865634918213\n",
      "Average Test Loss: 0.761683464050293\n",
      "Epoch: 82\n",
      "Epoch: 83\n",
      "Epoch: 84\n",
      "Epoch: 85\n",
      "Epoch: 86\n",
      "Epoch: 87\n",
      "Epoch: 88\n",
      "Epoch: 89\n",
      "Epoch: 90\n",
      "Epoch: 91\n",
      "Average Train Loss: 2.1457979679107666\n",
      "Average Test Loss: 0.769660472869873\n",
      "Epoch: 92\n",
      "Epoch: 93\n",
      "Epoch: 94\n",
      "Epoch: 95\n",
      "Epoch: 96\n",
      "Epoch: 97\n",
      "Epoch: 98\n",
      "Epoch: 99\n",
      "Epoch: 100\n",
      "Epoch: 101\n",
      "Average Train Loss: 2.2056427001953125\n",
      "Average Test Loss: 0.7701881527900696\n"
     ]
    }
   ],
   "source": [
    "w_h = init_weights((784, 625))\n",
    "w_h2 = init_weights((625, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])\n",
    "p_drop = 0.5\n",
    "\n",
    "# put this into a training loop over 100 epochs\n",
    "for i in range(101):\n",
    "    print(\"Epoch: {}\".format(i+1))\n",
    "    avg_train_loss = 0.\n",
    "    for (j, (X, y)) in enumerate(dataloader):\n",
    "        noise_py_x = dropout_model(X.reshape(mb_size, 784).to(cuda0), w_h, w_h2, w_o, p_drop, p_drop)\n",
    "        optimizer.zero_grad()\n",
    "        # the cross-entropy loss function already contains the softmax\n",
    "        cost = torch.nn.functional.cross_entropy(noise_py_x, y.to(cuda0), reduction=\"mean\")\n",
    "        avg_train_loss += cost\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(\"Average Train Loss: {}\".format(avg_train_loss / (j + 1)))\n",
    "\n",
    "        # no need to calculate gradients for validation\n",
    "        with torch.no_grad():\n",
    "            avg_test_loss = 0.\n",
    "            for (k, (X, y)) in enumerate(test_loader):\n",
    "                noise_py_x = model(X.reshape(mb_size, 784).to(cuda0), w_h, w_h2, w_o)\n",
    "                cost = torch.nn.functional.cross_entropy(noise_py_x, y.to(cuda0), reduction=\"mean\")\n",
    "                avg_test_loss += cost\n",
    "\n",
    "            print(\"Average Test Loss: {}\".format(avg_test_loss / (k + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZgRMmiI_Hqq"
   },
   "source": [
    "As expected the test loss with dropout is smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "it8QRhwg_Hqq"
   },
   "source": [
    "# Parametric Relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-weight:bold\">\n",
    "Comment: You use the same mask three times, which could be cached for efficiency. Also you reassign values from the input which, according to the sample solution, gives an error. Otherwise, 'PRelu' is implemented differently but gives the same results.\n",
    "<br>\n",
    "In the 'prelu_model' you didn't include dropout, but this wasn't required. This could be the reason though, that your PRelu-model overfits \n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gfJXkpls_Hqq"
   },
   "outputs": [],
   "source": [
    "def PRelu(X, a):\n",
    "    X[X <= 0] = X[X <= 0] * a.repeat(X.shape[0], 1)[X <= 0]\n",
    "    return X\n",
    "\n",
    "\n",
    "def prelu_model(X, w_h, w_h2, w_o, a):\n",
    "    h = PRelu(X @ w_h, a[0])\n",
    "    h2 = PRelu(h @ w_h2, a[1])\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7xY0GO7_Hqr",
    "outputId": "294fdece-61ac-4a38-d22d-c5f27259711a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Average Train Loss: 0.8047875761985779\n",
      "Average Test Loss: 0.4409887194633484\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Average Train Loss: 0.16160310804843903\n",
      "Average Test Loss: 0.3577435612678528\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Average Train Loss: 0.13123373687267303\n",
      "Average Test Loss: 0.30757373571395874\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Average Train Loss: 0.10430516302585602\n",
      "Average Test Loss: 0.5955327749252319\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Epoch: 41\n",
      "Average Train Loss: 0.08805377781391144\n",
      "Average Test Loss: 0.40722155570983887\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Epoch: 51\n",
      "Average Train Loss: 0.07319553196430206\n",
      "Average Test Loss: 0.7236766815185547\n",
      "Epoch: 52\n",
      "Epoch: 53\n",
      "Epoch: 54\n",
      "Epoch: 55\n",
      "Epoch: 56\n",
      "Epoch: 57\n",
      "Epoch: 58\n",
      "Epoch: 59\n",
      "Epoch: 60\n",
      "Epoch: 61\n",
      "Average Train Loss: 0.06539663672447205\n",
      "Average Test Loss: 0.7369979619979858\n",
      "Epoch: 62\n",
      "Epoch: 63\n",
      "Epoch: 64\n",
      "Epoch: 65\n",
      "Epoch: 66\n",
      "Epoch: 67\n",
      "Epoch: 68\n",
      "Epoch: 69\n",
      "Epoch: 70\n",
      "Epoch: 71\n",
      "Average Train Loss: 0.06062693893909454\n",
      "Average Test Loss: 0.7540104389190674\n",
      "Epoch: 72\n",
      "Epoch: 73\n",
      "Epoch: 74\n",
      "Epoch: 75\n",
      "Epoch: 76\n",
      "Epoch: 77\n",
      "Epoch: 78\n",
      "Epoch: 79\n",
      "Epoch: 80\n",
      "Epoch: 81\n",
      "Average Train Loss: 0.055435050278902054\n",
      "Average Test Loss: 0.6864425539970398\n",
      "Epoch: 82\n",
      "Epoch: 83\n",
      "Epoch: 84\n",
      "Epoch: 85\n",
      "Epoch: 86\n",
      "Epoch: 87\n",
      "Epoch: 88\n",
      "Epoch: 89\n",
      "Epoch: 90\n",
      "Epoch: 91\n",
      "Average Train Loss: 0.04944664612412453\n",
      "Average Test Loss: 0.6486156582832336\n",
      "Epoch: 92\n",
      "Epoch: 93\n",
      "Epoch: 94\n",
      "Epoch: 95\n",
      "Epoch: 96\n",
      "Epoch: 97\n",
      "Epoch: 98\n",
      "Epoch: 99\n",
      "Epoch: 100\n",
      "Epoch: 101\n",
      "Average Train Loss: 0.04518265649676323\n",
      "Average Test Loss: 0.8364734649658203\n"
     ]
    }
   ],
   "source": [
    "a = init_weights((2, 625))\n",
    "w_h = init_weights((784, 625))\n",
    "w_h2 = init_weights((625, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o, a])\n",
    "\n",
    "\n",
    "# put this into a training loop over 100 epochs\n",
    "for i in range(101):\n",
    "    print(\"Epoch: {}\".format(i+1))\n",
    "    avg_train_loss = 0.\n",
    "    for (j, (X, y)) in enumerate(dataloader):\n",
    "        noise_py_x = prelu_model(X.reshape(mb_size, 784).to(cuda0), w_h, w_h2, w_o, a)\n",
    "        optimizer.zero_grad()\n",
    "        # the cross-entropy loss function already contains the softmax\n",
    "        cost = torch.nn.functional.cross_entropy(noise_py_x, y.to(cuda0), reduction=\"mean\")\n",
    "        avg_train_loss += cost\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(\"Average Train Loss: {}\".format(avg_train_loss / (j + 1)))\n",
    "\n",
    "        # no need to calculate gradients for validation\n",
    "        with torch.no_grad():\n",
    "            avg_test_loss = 0.\n",
    "            for (k, (X, y)) in enumerate(test_loader):\n",
    "                noise_py_x = prelu_model(X.reshape(mb_size, 784).to(cuda0), w_h, w_h2, w_o, a)\n",
    "                cost = torch.nn.functional.cross_entropy(noise_py_x, y.to(cuda0), reduction=\"mean\")\n",
    "                avg_test_loss += cost\n",
    "\n",
    "            print(\"Average Test Loss: {}\".format(avg_test_loss / (k + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAKod9bQhlv3"
   },
   "source": [
    "The resulting loss is smaller than the plain ReLu, but looks like of an improvement than the dropout method. But one would need to gather some statistic with more training runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhaNhO_s_Hqs"
   },
   "source": [
    "# 4 Convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-weight:bold\">\n",
    "Comment: The convolutional network is correct, but most of task 4.2 is missing, i.e. the sketch of the cnn, the plots and the second part of 4.2\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "w88g7Hpu_Hqs"
   },
   "outputs": [],
   "source": [
    "def convolutional_network(X, w_c1, w_c2, w_c3, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "\n",
    "    X = X.reshape(-1, 1, 28, 28)\n",
    "    \n",
    "    con_l1 = convolutional_layer(X, w_c1, p_drop_input)\n",
    "    con_l2 = convolutional_layer(con_l1, w_c2, p_drop_input)\n",
    "    con_l3 = convolutional_layer(con_l2, w_c3, p_drop_input)\n",
    "    \n",
    "    con_l3 = con_l3.reshape(con_l3.shape[0], -1)\n",
    "\n",
    "    h2 = rectify(dropout(con_l3, p_drop_hidden) @ w_h2)\n",
    "    pre_softmax = dropout(h2, p_drop_hidden)  @ w_o\n",
    "    \n",
    "    return pre_softmax\n",
    "\n",
    "def convolutional_layer(input_layer, weights, p_drop):\n",
    "    \n",
    "    con_layer = rectify(conv2d(input_layer, weights))\n",
    "    subsampling_layer = max_pool2d(con_layer, (2, 2))\n",
    "    out_layer = dropout(subsampling_layer, p_drop)\n",
    "    \n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KUOT2azt_Hqs"
   },
   "outputs": [],
   "source": [
    "w_c1 = init_weights((32, 1, 5, 5))\n",
    "w_c2 = init_weights((64, 32, 5, 5))\n",
    "w_c3 = init_weights((128, 64, 2, 2))\n",
    "\n",
    "number_of_output_pixel = 128\n",
    "\n",
    "w_h2 = init_weights((number_of_output_pixel, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "optimizer = RMSprop([w_c1, w_c2, w_c3, w_h2, w_o])\n",
    "p_drop = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_zDn4OCB_Hqt",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Average Train Loss: 6.98406982421875\n",
      "Average Test Loss: 2.3138933181762695\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Average Train Loss: 2.3272807598114014\n",
      "Average Test Loss: 2.3050177097320557\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Average Train Loss: 2.3267812728881836\n",
      "Average Test Loss: 2.370023727416992\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Average Train Loss: 2.331801414489746\n",
      "Average Test Loss: 2.3651843070983887\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Epoch: 41\n",
      "Average Train Loss: 2.3353028297424316\n",
      "Average Test Loss: 2.2922468185424805\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Epoch: 51\n",
      "Average Train Loss: 2.343891143798828\n",
      "Average Test Loss: 2.3128747940063477\n",
      "Epoch: 52\n",
      "Epoch: 53\n",
      "Epoch: 54\n",
      "Epoch: 55\n",
      "Epoch: 56\n",
      "Epoch: 57\n",
      "Epoch: 58\n",
      "Epoch: 59\n",
      "Epoch: 60\n",
      "Epoch: 61\n",
      "Average Train Loss: 2.3683886528015137\n",
      "Average Test Loss: 2.3111608028411865\n",
      "Epoch: 62\n",
      "Epoch: 63\n",
      "Epoch: 64\n",
      "Epoch: 65\n",
      "Epoch: 66\n",
      "Epoch: 67\n",
      "Epoch: 68\n",
      "Epoch: 69\n",
      "Epoch: 70\n",
      "Epoch: 71\n",
      "Average Train Loss: 2.3373632431030273\n",
      "Average Test Loss: 2.3557138442993164\n",
      "Epoch: 72\n",
      "Epoch: 73\n",
      "Epoch: 74\n",
      "Epoch: 75\n",
      "Epoch: 76\n",
      "Epoch: 77\n",
      "Epoch: 78\n",
      "Epoch: 79\n",
      "Epoch: 80\n",
      "Epoch: 81\n",
      "Average Train Loss: 2.3182849884033203\n",
      "Average Test Loss: 2.4144539833068848\n",
      "Epoch: 82\n",
      "Epoch: 83\n",
      "Epoch: 84\n",
      "Epoch: 85\n",
      "Epoch: 86\n",
      "Epoch: 87\n",
      "Epoch: 88\n",
      "Epoch: 89\n",
      "Epoch: 90\n",
      "Epoch: 91\n",
      "Average Train Loss: 2.3576083183288574\n",
      "Average Test Loss: 2.342223644256592\n",
      "Epoch: 92\n",
      "Epoch: 93\n",
      "Epoch: 94\n",
      "Epoch: 95\n",
      "Epoch: 96\n",
      "Epoch: 97\n",
      "Epoch: 98\n",
      "Epoch: 99\n",
      "Epoch: 100\n",
      "Epoch: 101\n",
      "Average Train Loss: 2.3561508655548096\n",
      "Average Test Loss: 2.3632707595825195\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for i in range(101):\n",
    "    print(\"Epoch: {}\".format(i+1))\n",
    "    avg_train_loss = 0.\n",
    "    for (j, (X, y)) in enumerate(dataloader):\n",
    "      \n",
    "        noise_py_x = convolutional_network(X.reshape(mb_size, 784).to(cuda0), w_c1, w_c2, w_c3, w_h2, w_o, p_drop, p_drop)\n",
    "        optimizer.zero_grad()\n",
    "        # the cross-entropy loss function already contains the softmax\n",
    "        cost = torch.nn.functional.cross_entropy(noise_py_x.to(cuda0), y.to(cuda0), reduction=\"mean\")\n",
    "        avg_train_loss += cost\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(\"Average Train Loss: {}\".format(avg_train_loss / (j + 1)))\n",
    "\n",
    "        # no need to calculate gradients for validation\n",
    "        with torch.no_grad():\n",
    "            avg_test_loss = 0.\n",
    "            for (k, (X, y)) in enumerate(test_loader):\n",
    "                noise_py_x = convolutional_network(X.reshape(mb_size, 784).to(cuda0), w_c1, w_c2, w_c3, w_h2, w_o, p_drop, p_drop)\n",
    "                cost = torch.nn.functional.cross_entropy(noise_py_x, y.to(cuda0), reduction=\"mean\")\n",
    "                avg_test_loss += cost\n",
    "\n",
    "            print(\"Average Test Loss: {}\".format(avg_test_loss / (k + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Shuq8XhbIvR1"
   },
   "source": [
    "The implementation seems to be correct, but we had some troubles succesfully training the network. Even using a GPU (google collab), the loss did not seem to reduce enough. The training took extremely long compared to before which made it difficult to identify the mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
