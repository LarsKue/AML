{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Exercise 3\n",
    "### Lars Kuehmichel, Nicolas Wolf\n",
    "\n",
    "### 1 Introduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mb_size = 100 # mini-batch size of 100\n",
    "n_epochs = 101\n",
    "\n",
    "\n",
    "trans = transforms.Compose([transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "\n",
    "dataset = dset.MNIST(\"./\", download = True,\n",
    "                     train = True,\n",
    "                     transform = trans)\n",
    "\n",
    "test_dataset = dset.MNIST(\"./\", download=True,\n",
    "                          train=False,\n",
    "                          transform = trans)\n",
    "\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=mb_size,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=mb_size,\n",
    "                                          shuffle=True, num_workers=1,\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    # Kaiming He initialization (a good initialization is important)\n",
    "    # https://arxiv.org/abs/1502.01852\n",
    "    std = np.sqrt(2. / shape[0])\n",
    "    w = torch.randn(size=shape) * std\n",
    "    w.requires_grad = True\n",
    "    return w\n",
    "\n",
    "\n",
    "def rectify(X):\n",
    "    return torch.max(torch.zeros_like(X), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# this is an example as a reduced version of the pytorch internal RMSprop optimizer\n",
    "class RMSprop(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.5, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(grad, grad, value=1-alpha)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(grad, avg, value=-group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, train=False):\n",
    "    h = rectify(X @ w_h)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax\n",
    "\n",
    "\n",
    "w_h = init_weights((784, 625))\n",
    "w_h2 = init_weights((625, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, n_epochs, batch_size, model_args=()):\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    # put this into a training loop over n epochs\n",
    "    for i in range(n_epochs):\n",
    "        print(\"Epoch: {}\".format(i+1))\n",
    "        train_loss = 0\n",
    "        correct_train = []\n",
    "        test_loss = 0\n",
    "        correct_test = []\n",
    "        j = 0\n",
    "        for (j, (X, y)) in enumerate(dataloader):\n",
    "            noise_py_x = model(X.reshape(batch_size, -1), *model_args, train=True)\n",
    "            optimizer.zero_grad()\n",
    "            # the cross-entropy loss function already contains the softmax\n",
    "            cost = torch.nn.functional.cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "            train_loss += cost\n",
    "\n",
    "            train_labels = torch.argmax(noise_py_x, dim=-1)\n",
    "            correct = (y == train_labels).detach().numpy()\n",
    "            correct_train.append(correct)\n",
    "            cost.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # no need to calculate gradients for validation\n",
    "        with torch.no_grad():\n",
    "            k = 0\n",
    "            for (k, (X, y)) in enumerate(test_loader):\n",
    "                noise_py_x = model(X.reshape(batch_size, -1), *model_args, train=False)\n",
    "                cost = torch.nn.functional.cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "                test_loss += cost\n",
    "                test_labels = torch.argmax(noise_py_x, dim=-1)\n",
    "                correct = (y == test_labels).detach().numpy()\n",
    "                correct_test.append(correct)\n",
    "\n",
    "        train_losses.append(train_loss / (j + 1))\n",
    "        train_accuracies.append(np.mean(correct_train))\n",
    "        test_losses.append(test_loss / (k + 1))\n",
    "        test_accuracies.append(np.mean(correct_test))\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Average Train Loss: {sum(train_losses) / (i + 1)}\")\n",
    "            print(f\"Average Test Loss: {sum(test_losses) / (i + 1)}\")\n",
    "\n",
    "    return train_losses, train_accuracies, test_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Average Train Loss: 0.4020918011665344\n",
      "Average Test Loss: 0.19562608003616333\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Average Train Loss: 0.19959086179733276\n",
      "Average Test Loss: 0.2270035594701767\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Average Train Loss: 0.16350901126861572\n",
      "Average Test Loss: 0.3037390410900116\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Average Train Loss: 0.13900905847549438\n",
      "Average Test Loss: 0.3588823974132538\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_accuracy, test_loss, test_accuracy = train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=mb_size,\n",
    "    model_args=(w_h, w_h2, w_o)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "epochs = 1 + np.arange(len(train_loss))\n",
    "\n",
    "_, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "axes[0].plot(epochs, train_loss, label=\"Train\")\n",
    "axes[0].plot(epochs, test_loss, label=\"Test\")\n",
    "axes[1].plot(epochs, train_accuracy, label=\"Train\")\n",
    "axes[1].plot(epochs, test_accuracy, label=\"Test\")\n",
    "\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_yscale(\"log\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2 Dropout\n",
    "\n",
    "Dropout randomly sets parts of the network to inactive (i.e. zero) during training.\n",
    "This means that the network cannot train to overly rely on very specific neuron connections,\n",
    "thus reducing overfitting.\n",
    "\n",
    "We can see that without dropout, the training loss steadily decreases, whereas the validation\n",
    "loss increases after the first few epochs. This means the model is overfitting.\n",
    "\n",
    "With dropout, the training loss does not decrease as much, and even slightly increases after\n",
    "the first few epochs. This is likely because the network is locally diverging, and could be improved\n",
    "by scheduling the learning rate. However, the validation\n",
    "loss stays low throughout training, meaning the model does not overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def dropout(X, p_drop=0.5, train=False):\n",
    "    # dropout is disabled in validation\n",
    "    if train and (0 < p_drop < 1):\n",
    "        # rand_like gives errors for some dtypes\n",
    "        r = torch.rand(X.shape)\n",
    "        keep = r >= p_drop\n",
    "        return (X * keep) / (1 - p_drop)\n",
    "    return X\n",
    "\n",
    "# test this\n",
    "X = torch.tensor([\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10, 11, 12]\n",
    "])\n",
    "\n",
    "X = dropout(X)\n",
    "\n",
    "# works fine\n",
    "print(X)\n",
    "\n",
    "\n",
    "def dropout_model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden, train=False):\n",
    "    h = dropout(X @ w_h, p_drop_input, train=train)\n",
    "    h2 = rectify(h)\n",
    "    h3 = dropout(h2 @ w_h2, p_drop_hidden, train=train)\n",
    "    h4 = rectify(h3)\n",
    "    h5 = dropout(h4 @ w_o, p_drop_hidden, train=train)\n",
    "    pre_softmax = h5\n",
    "    return pre_softmax\n",
    "\n",
    "p_drop_input = 0.5\n",
    "p_drop_hidden = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Training the Dropout Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "train_loss_d, train_accuracy_d, test_loss_d, test_accuracy_d = train(\n",
    "    dropout_model,\n",
    "    optimizer,\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=mb_size,\n",
    "    model_args=(\n",
    "        w_h,\n",
    "        w_h2,\n",
    "        w_o,\n",
    "        p_drop_input,\n",
    "        p_drop_hidden,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "epochs = 1 + np.arange(len(train_loss_d))\n",
    "\n",
    "_, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "axes[0].plot(epochs, train_loss_d, label=\"Train\")\n",
    "axes[0].plot(epochs, test_loss_d, label=\"Test\")\n",
    "axes[1].plot(epochs, train_accuracy_d, label=\"Train\")\n",
    "axes[1].plot(epochs, test_accuracy_d, label=\"Test\")\n",
    "\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_yscale(\"log\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3 Parametric ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def PRelu(X, a):\n",
    "    return torch.where(X > 0, X, a * X)\n",
    "\n",
    "\n",
    "def prelu_model(X, a, w_h, w_h2, w_o, p_drop_input, p_drop_hidden, train=False):\n",
    "    h = dropout(X @ w_h, p_drop_input, train=train)\n",
    "    h2 = PRelu(h, a)\n",
    "    h3 = dropout(h2 @ w_h2, p_drop_hidden, train=train)\n",
    "    h4 = PRelu(h3, a)\n",
    "    h5 = dropout(h4 @ w_o, p_drop_hidden, train=train)\n",
    "    pre_softmax = h5\n",
    "    return pre_softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "a = init_weights((1, 625))\n",
    "param_optimizer = RMSprop([a, w_h, w_h2, w_o])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "train_loss_p, train_accuracy_p, test_loss_p, test_accuracy_p = train(\n",
    "    prelu_model,\n",
    "    param_optimizer,\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=mb_size,\n",
    "    model_args=(\n",
    "        a,\n",
    "        w_h,\n",
    "        w_h2,\n",
    "        w_o,\n",
    "        p_drop_input,\n",
    "        p_drop_hidden,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "epochs = 1 + np.arange(len(train_loss_p))\n",
    "\n",
    "_, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "axes[0].plot(epochs, train_loss_p, label=\"Train\")\n",
    "axes[0].plot(epochs, test_loss_p, label=\"Test\")\n",
    "axes[1].plot(epochs, train_accuracy_p, label=\"Train\")\n",
    "axes[1].plot(epochs, test_accuracy_p, label=\"Test\")\n",
    "\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_yscale(\"log\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4 Convolutional Layers\n",
    "\n",
    "#### 4.1 Create a Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def custom_conv2d(in_channels, out_channels, kernel_size, p_dropout):\n",
    "    # define a custom convolutional layer with\n",
    "    # dropout, conv2d, prelu activation and maxpooling\n",
    "    # also takes care of weight initialization\n",
    "    drop = torch.nn.Dropout(p=p_dropout)\n",
    "    conv = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size)\n",
    "    conv.weight = torch.nn.Parameter(init_weights((out_channels, in_channels, *kernel_size)))\n",
    "    prelu = torch.nn.PReLU(num_parameters=out_channels)\n",
    "    prelu.weight = torch.nn.Parameter(init_weights((out_channels,)))\n",
    "    # the sample code implicitly sets stride=(2, 2)\n",
    "    maxpool = torch.nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "    return torch.nn.Sequential(\n",
    "        drop,\n",
    "        conv,\n",
    "        prelu,\n",
    "        maxpool\n",
    "    )\n",
    "\n",
    "def dense(in_features, out_features):\n",
    "    # same as custom_conv2d but for fully connected layers\n",
    "    linear = torch.nn.Linear(in_features=in_features, out_features=out_features)\n",
    "    prelu = torch.nn.PReLU(num_parameters=out_features)\n",
    "    prelu.weight = torch.nn.Parameter(init_weights((out_features,)))\n",
    "\n",
    "    return torch.nn.Sequential(\n",
    "        linear,\n",
    "        prelu\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 128 filters, 1x1 image\n",
    "n_out_px = 128\n",
    "\n",
    "p_drop_input = 0.25\n",
    "p_drop_hidden = 0.25\n",
    "\n",
    "# in_features are given by the preceding layer\n",
    "conv_model = torch.nn.Sequential(\n",
    "    custom_conv2d(in_channels=1, out_channels=32,\n",
    "                  kernel_size=(5, 5), p_dropout=p_drop_input),\n",
    "    custom_conv2d(in_channels=32, out_channels=64,\n",
    "                  kernel_size=(5, 5), p_dropout=p_drop_hidden),\n",
    "    custom_conv2d(in_channels=64, out_channels=128,\n",
    "                  kernel_size=(2, 2), p_dropout=p_drop_hidden),\n",
    "    torch.nn.Flatten(),\n",
    "    dense(in_features=n_out_px, out_features=784),\n",
    "    # same layer sizes as before\n",
    "    dense(in_features=784, out_features=625),\n",
    "    dense(in_features=625, out_features=625),\n",
    "    # linear (pre-softmax) output layer\n",
    "    torch.nn.Linear(in_features=625, out_features=10)\n",
    ")\n",
    "\n",
    "conv_optimizer = torch.optim.RMSprop(\n",
    "    conv_model.parameters(),\n",
    "    # ensure same hyper-parameters as before for comparability\n",
    "    lr=1e-3,\n",
    "    alpha=0.5,\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "conv_loss = torch.nn.CrossEntropyLoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def train_conv(model, optimizer, loss, n_epochs, shape):\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0\n",
    "        correct_train = []\n",
    "        test_loss = 0\n",
    "        correct_test = []\n",
    "\n",
    "        model.train(mode=True)\n",
    "        for i, (x, ystar) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            yhat = model(x.reshape(shape))\n",
    "            l = loss(yhat, ystar)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # add float value of the loss\n",
    "            train_loss += l.detach().item()\n",
    "            train_labels = torch.argmax(yhat, dim=-1)\n",
    "            correct = (ystar == train_labels).detach().numpy()\n",
    "            correct_train.append(correct)\n",
    "\n",
    "        model.train(mode=False)\n",
    "        with torch.no_grad():\n",
    "            for j, (x, ystar) in enumerate(test_loader):\n",
    "                yhat = model(x.reshape(shape))\n",
    "                l = loss(yhat, ystar)\n",
    "\n",
    "                test_loss += l.detach().item()\n",
    "                test_labels = torch.argmax(yhat, dim=-1)\n",
    "                correct = (ystar == test_labels).detach().numpy()\n",
    "                correct_test.append(correct)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1} / {n_epochs}\")\n",
    "\n",
    "        train_losses.append(train_loss / (i + 1))\n",
    "        train_accuracies.append(np.mean(correct_train))\n",
    "        test_losses.append(test_loss / (j + 1))\n",
    "        test_accuracies.append(np.mean(correct_test))\n",
    "    return train_losses, train_accuracies, test_losses, test_accuracies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.2 Application of Convolutional Network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "train_loss_c, train_accuracy_c, test_loss_c, test_accuracy_c = train_conv(\n",
    "    conv_model,\n",
    "    conv_optimizer,\n",
    "    conv_loss,\n",
    "    n_epochs=n_epochs,\n",
    "    shape=(-1, 1, 28, 28)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "epochs = 1 + np.arange(len(train_loss_c))\n",
    "\n",
    "_, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "axes[0].plot(epochs, train_loss_c, label=\"Train\")\n",
    "axes[0].plot(epochs, test_loss_c, label=\"Test\")\n",
    "axes[1].plot(epochs, train_accuracy_c, label=\"Train\")\n",
    "axes[1].plot(epochs, test_accuracy_c, label=\"Test\")\n",
    "\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_yscale(\"log\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Visualizations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_image, test_label = next(iter(dataloader))\n",
    "\n",
    "# one image, one channel, 28x28 px\n",
    "test_image = test_image[0].reshape(1, 1, 28, 28)\n",
    "test_label = test_label[0].item()\n",
    "print(\"Target Label:\", test_label)\n",
    "\n",
    "plt.figure(figsize=(10, 9))\n",
    "# only the 28x28 px information here\n",
    "plt.imshow(test_image[0][0], cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Filtered Images\n",
    "\n",
    "Images where the filter responds to a single pixel are very sharp.\n",
    "\n",
    "Images where the filter responds to a region of pixels are blurry.\n",
    "\n",
    "Images are inverted if the filter is inverted."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "modules = [module for module in conv_model.modules() if not isinstance(module, torch.nn.Sequential)]\n",
    "\n",
    "convolution = modules[1]\n",
    "\n",
    "convoluted_image = convolution(test_image)\n",
    "\n",
    "plt.figure(figsize=(10, 9))\n",
    "for i in range(9):\n",
    "    img = convoluted_image[0][i].detach().numpy()\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Corresponding Filter Weights\n",
    "\n",
    "Most filters heavily rely on a single pixel of the input image.\n",
    "Some rely heavily on a single region of pixels.\n",
    "\n",
    "Filters that look \"inverted\" with respect to one another may have\n",
    "the same effect, with negative weights in the next layer.\n",
    "More explicitly, it does not matter if the filter is black with a\n",
    "white pixel, or white with a black pixel, since this only changes\n",
    "the sign of the filter's output."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 9))\n",
    "for i in range(9):\n",
    "    img = convolution.weight[i][0].detach().numpy()\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Modifying the Network\n",
    "\n",
    "We decided to increase the filter size of the first convolutional layer.\n",
    "\n",
    "In order to still have a nonzero number of pixels for the last convolutional layer, the size\n",
    "of the intermediate layer had to be reduced."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 128 channels, 1x1 image\n",
    "n_out_px = 128\n",
    "\n",
    "modified_conv_model = torch.nn.Sequential(\n",
    "    custom_conv2d(in_channels=1, out_channels=32,\n",
    "                  kernel_size=(8, 8), p_dropout=p_drop_input),\n",
    "    custom_conv2d(in_channels=32, out_channels=64,\n",
    "                  kernel_size=(4, 4), p_dropout=p_drop_hidden),\n",
    "    custom_conv2d(in_channels=64, out_channels=128,\n",
    "                  kernel_size=(2, 2), p_dropout=p_drop_hidden),\n",
    "    torch.nn.Flatten(),\n",
    "    dense(in_features=n_out_px, out_features=784),\n",
    "    # same layer sizes as before\n",
    "    dense(in_features=784, out_features=625),\n",
    "    dense(in_features=625, out_features=625),\n",
    "    # linear (pre-softmax) output layer\n",
    "    torch.nn.Linear(in_features=625, out_features=10)\n",
    ")\n",
    "\n",
    "modified_conv_optimizer = torch.optim.RMSprop(\n",
    "    modified_conv_model.parameters(),\n",
    "    # ensure same hyper-parameters as before for comparability\n",
    "    lr=1e-3,\n",
    "    alpha=0.5,\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "modified_conv_loss = conv_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loss_mc, train_accuracy_mc, test_loss_mc, test_accuracy_mc = train_conv(\n",
    "    modified_conv_model,\n",
    "    modified_conv_optimizer,\n",
    "    modified_conv_loss,\n",
    "    n_epochs=n_epochs,\n",
    "    shape=(-1, 1, 28, 28)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 1 + np.arange(len(train_loss_mc))\n",
    "\n",
    "_, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "axes[0].plot(epochs, train_loss_mc, label=\"Train\")\n",
    "axes[0].plot(epochs, test_loss_mc, label=\"Test\")\n",
    "axes[1].plot(epochs, train_accuracy_mc, label=\"Train\")\n",
    "axes[1].plot(epochs, test_accuracy_mc, label=\"Test\")\n",
    "\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_yscale(\"log\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Visualizing the Modified Network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "modules = [module for module in modified_conv_model.modules() if not isinstance(module, torch.nn.Sequential)]\n",
    "\n",
    "convolution = modules[1]\n",
    "\n",
    "convoluted_image = convolution(test_image)\n",
    "\n",
    "plt.figure(figsize=(10, 9))\n",
    "for i in range(9):\n",
    "    img = convoluted_image[0][i].detach().numpy()\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Filters tend to be more complex than before. Instead of activating strongly on single pixels,\n",
    "we can see more complex shapes here, like the back arch of a 7, or part of a 0. This means the\n",
    "filters will pick up more macroscopic features in the images."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 9))\n",
    "for i in range(9):\n",
    "    img = convolution.weight[i][0].detach().numpy()\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Error and Loss Overview"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "line_names = [\"Train\", \"Test\"]\n",
    "column_names = [\"MLP\", \"Dropout\", \"Parameterized\", \"Convolutional\", \"Modified Convolutional\"]\n",
    "\n",
    "accuracies = np.array([\n",
    "    [train_accuracy[-1], train_accuracy_d[-1], train_accuracy_p[-1], train_accuracy_c[-1], train_accuracy_mc[-1]],\n",
    "    [test_accuracy[-1], test_accuracy_d[-1], test_accuracy_p[-1], test_accuracy_c[-1], test_accuracy_mc[-1]]\n",
    "])\n",
    "\n",
    "losses = np.array([\n",
    "    [train_loss[-1], train_loss_d[-1], train_loss_p[-1], train_loss_c[-1], train_loss_mc[-1]],\n",
    "    [test_loss[-1], test_loss_d[-1], test_loss_p[-1], test_loss_c[-1], test_loss_mc[-1]]\n",
    "])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame(losses)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracy_df = pd.DataFrame(accuracies)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}